{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place du context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from findspark import init\n",
    "init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contexte est récupéré si existant et dans l'autre cas il sera créé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate() # ça marche pas pour moi avec sparkContext\n",
    "spark = SparkSession.builder.getOrCreate() # Du coup je fais une sparkSession (ça doit revenir presque au même je pense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "from os import makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"res\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-29 21:50:03--  https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\n",
      "Résolution de s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)… 52.218.36.34\n",
      "Connexion à s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.36.34|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 4576820670 (4,3G) [application/x-gzip]\n",
      "Enregistre : «dac.tar.gz»\n",
      "\n",
      "dac.tar.gz            0%[                    ]   7,66M   929KB/s    tps 81m 14s^C\n"
     ]
    }
   ],
   "source": [
    "if not path.isfile(\"dac.tar.gz\"):\n",
    "    !wget https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.isdir(data_dir):\n",
    "    makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier [`dac.tar.gz`](https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz) doit se situer dans le même répertoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.isfile(data_dir + \"/train.txt\") or not path.isfile(data_dir + \"/test.txt\"):\n",
    "    !tar -xvzf dac.tar.gz -C $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(data_dir + \"/train.txt\", header=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45840617\n",
      "756554\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true),StructField(_c3,StringType,true),StructField(_c4,StringType,true),StructField(_c5,StringType,true),StructField(_c6,StringType,true),StructField(_c7,StringType,true),StructField(_c8,StringType,true),StructField(_c9,StringType,true),StructField(_c10,StringType,true),StructField(_c11,StringType,true),StructField(_c12,StringType,true),StructField(_c13,StringType,true),StructField(_c14,StringType,true),StructField(_c15,StringType,true),StructField(_c16,StringType,true),StructField(_c17,StringType,true),StructField(_c18,StringType,true),StructField(_c19,StringType,true),StructField(_c20,StringType,true),StructField(_c21,StringType,true),StructField(_c22,StringType,true),StructField(_c23,StringType,true),StructField(_c24,StringType,true),StructField(_c25,StringType,true),StructField(_c26,StringType,true),StructField(_c27,StringType,true),StructField(_c28,StringType,true),StructField(_c29,StringType,true),StructField(_c30,StringType,true),StructField(_c31,StringType,true),StructField(_c32,StringType,true),StructField(_c33,StringType,true),StructField(_c34,StringType,true),StructField(_c35,StringType,true),StructField(_c36,StringType,true),StructField(_c37,StringType,true),StructField(_c38,StringType,true),StructField(_c39,StringType,true)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir les colonne 0 à 13 inclu en float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.select(*(col(c).cast(\"float\") if i < 14 else col(c) for i, c in enumerate(df2.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renommage des colonnes par : label, f1, f2, f3,..., f39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39']\n"
     ]
    }
   ],
   "source": [
    "def mk_newNameCol():\n",
    "    res = ['label']\n",
    "    for i in range(1, 40):\n",
    "        res.append('f' + str(i))\n",
    "    return res\n",
    "\n",
    "new_name = mk_newNameCol()\n",
    "print(new_name)\n",
    "\n",
    "df3 = df3.select(*[col(c).alias(new_c) for c, new_c in zip(df3.columns, new_name)])\n",
    "# df3.head() # Ca bug à ce moment là je sais pas pourquoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les valeurs uniques pour les attributs 14 à 39 inclu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`_c14`' given input columns: [label];;\\n'Project ['_c14, '_c15, '_c16, '_c17, '_c18, '_c19, '_c20, '_c21, '_c22, '_c23, '_c24, '_c25, '_c26, '_c27, '_c28, '_c29, '_c30, '_c31, '_c32, '_c33, '_c34, '_c35, '_c36, '_c37, ... 2 more fields]\\n+- Project [label#633 AS label#638]\\n   +- Project [_c0#135 AS label#633]\\n      +- Project [cast(_c0#12 as float) AS _c0#135, cast(_c1#13 as float) AS _c1#136, cast(_c2#14 as float) AS _c2#137, cast(_c3#15 as float) AS _c3#138, cast(_c4#16 as float) AS _c4#139, cast(_c5#17 as float) AS _c5#140, cast(_c6#18 as float) AS _c6#141, cast(_c7#19 as float) AS _c7#142, cast(_c8#20 as float) AS _c8#143, cast(_c9#21 as float) AS _c9#144, cast(_c10#22 as float) AS _c10#145, cast(_c11#23 as float) AS _c11#146, cast(_c12#24 as float) AS _c12#147, cast(_c13#25 as float) AS _c13#148, _c14#26, _c15#27, _c16#28, _c17#29, _c18#30, _c19#31, _c20#32, _c21#33, _c22#34, _c23#35, ... 16 more fields]\\n         +- Filter AtLeastNNulls(n, _c0#12,_c1#13,_c2#14,_c3#15,_c4#16,_c5#17,_c6#18,_c7#19,_c8#20,_c9#21,_c10#22,_c11#23,_c12#24,_c13#25,_c14#26,_c15#27,_c16#28,_c17#29,_c18#30,_c19#31,_c20#32,_c21#33,_c22#34,_c23#35,_c24#36,_c25#37,_c26#38,_c27#39,_c28#40,_c29#41,_c30#42,_c31#43,_c32#44,_c33#45,_c34#46,_c35#47,_c36#48,_c37#49,_c38#50,_c39#51)\\n            +- Relation[_c0#12,_c1#13,_c2#14,_c3#15,_c4#16,_c5#17,_c6#18,_c7#19,_c8#20,_c9#21,_c10#22,_c11#23,_c12#24,_c13#25,_c14#26,_c15#27,_c16#28,_c17#29,_c18#30,_c19#31,_c20#32,_c21#33,_c22#34,_c23#35,... 16 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o278.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`_c14`' given input columns: [label];;\n'Project ['_c14, '_c15, '_c16, '_c17, '_c18, '_c19, '_c20, '_c21, '_c22, '_c23, '_c24, '_c25, '_c26, '_c27, '_c28, '_c29, '_c30, '_c31, '_c32, '_c33, '_c34, '_c35, '_c36, '_c37, ... 2 more fields]\n+- Project [label#633 AS label#638]\n   +- Project [_c0#135 AS label#633]\n      +- Project [cast(_c0#12 as float) AS _c0#135, cast(_c1#13 as float) AS _c1#136, cast(_c2#14 as float) AS _c2#137, cast(_c3#15 as float) AS _c3#138, cast(_c4#16 as float) AS _c4#139, cast(_c5#17 as float) AS _c5#140, cast(_c6#18 as float) AS _c6#141, cast(_c7#19 as float) AS _c7#142, cast(_c8#20 as float) AS _c8#143, cast(_c9#21 as float) AS _c9#144, cast(_c10#22 as float) AS _c10#145, cast(_c11#23 as float) AS _c11#146, cast(_c12#24 as float) AS _c12#147, cast(_c13#25 as float) AS _c13#148, _c14#26, _c15#27, _c16#28, _c17#29, _c18#30, _c19#31, _c20#32, _c21#33, _c22#34, _c23#35, ... 16 more fields]\n         +- Filter AtLeastNNulls(n, _c0#12,_c1#13,_c2#14,_c3#15,_c4#16,_c5#17,_c6#18,_c7#19,_c8#20,_c9#21,_c10#22,_c11#23,_c12#24,_c13#25,_c14#26,_c15#27,_c16#28,_c17#29,_c18#30,_c19#31,_c20#32,_c21#33,_c22#34,_c23#35,_c24#36,_c25#37,_c26#38,_c27#39,_c28#40,_c29#41,_c30#42,_c31#43,_c32#44,_c33#45,_c34#46,_c35#47,_c36#48,_c37#49,_c38#50,_c39#51)\n            +- Relation[_c0#12,_c1#13,_c2#14,_c3#15,_c4#16,_c5#17,_c6#18,_c7#19,_c8#20,_c9#21,_c10#22,_c11#23,_c12#24,_c13#25,_c14#26,_c15#27,_c16#28,_c17#29,_c18#30,_c19#31,_c20#32,_c21#33,_c22#34,_c23#35,... 16 more fields] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2872)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1153)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5e277f47ff9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# marche pas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \"\"\"\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`_c14`' given input columns: [label];;\\n'Project ['_c14, '_c15, '_c16, '_c17, '_c18, '_c19, '_c20, '_c21, '_c22, '_c23, '_c24, '_c25, '_c26, '_c27, '_c28, '_c29, '_c30, '_c31, '_c32, '_c33, '_c34, '_c35, '_c36, '_c37, ... 2 more fields]\\n+- Project [label#633 AS label#638]\\n   +- Project [_c0#135 AS label#633]\\n      +- Project [cast(_c0#12 as float) AS _c0#135, cast(_c1#13 as float) AS _c1#136, cast(_c2#14 as float) AS _c2#137, cast(_c3#15 as float) AS _c3#138, cast(_c4#16 as float) AS _c4#139, cast(_c5#17 as float) AS _c5#140, cast(_c6#18 as float) AS _c6#141, cast(_c7#19 as float) AS _c7#142, cast(_c8#20 as float) AS _c8#143, cast(_c9#21 as float) AS _c9#144, cast(_c10#22 as float) AS _c10#145, cast(_c11#23 as float) AS _c11#146, cast(_c12#24 as float) AS _c12#147, cast(_c13#25 as float) AS _c13#148, _c14#26, _c15#27, _c16#28, _c17#29, _c18#30, _c19#31, _c20#32, _c21#33, _c22#34, _c23#35, ... 16 more fields]\\n         +- Filter AtLeastNNulls(n, _c0#12,_c1#13,_c2#14,_c3#15,_c4#16,_c5#17,_c6#18,_c7#19,_c8#20,_c9#21,_c10#22,_c11#23,_c12#24,_c13#25,_c14#26,_c15#27,_c16#28,_c17#29,_c18#30,_c19#31,_c20#32,_c21#33,_c22#34,_c23#35,_c24#36,_c25#37,_c26#38,_c27#39,_c28#40,_c29#41,_c30#42,_c31#43,_c32#44,_c33#45,_c34#46,_c35#47,_c36#48,_c37#49,_c38#50,_c39#51)\\n            +- Relation[_c0#12,_c1#13,_c2#14,_c3#15,_c4#16,_c5#17,_c6#18,_c7#19,_c8#20,_c9#21,_c10#22,_c11#23,_c12#24,_c13#25,_c14#26,_c15#27,_c16#28,_c17#29,_c18#30,_c19#31,_c20#32,_c21#33,_c22#34,_c23#35,... 16 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "df3.select(*(col(c) for c in df2.columns[14:])).distinct().show() # marche pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,FloatType,true),StructField(_c1,FloatType,true),StructField(_c2,FloatType,true),StructField(_c3,FloatType,true),StructField(_c4,FloatType,true),StructField(_c5,FloatType,true),StructField(_c6,FloatType,true),StructField(_c7,FloatType,true),StructField(_c8,FloatType,true),StructField(_c9,FloatType,true),StructField(_c10,FloatType,true),StructField(_c11,FloatType,true),StructField(_c12,FloatType,true),StructField(_c13,FloatType,true),StructField(_c14,StringType,true),StructField(_c15,StringType,true),StructField(_c16,StringType,true),StructField(_c17,StringType,true),StructField(_c18,StringType,true),StructField(_c19,StringType,true),StructField(_c20,StringType,true),StructField(_c21,StringType,true),StructField(_c22,StringType,true),StructField(_c23,StringType,true),StructField(_c24,StringType,true),StructField(_c25,StringType,true),StructField(_c26,StringType,true),StructField(_c27,StringType,true),StructField(_c28,StringType,true),StructField(_c29,StringType,true),StructField(_c30,StringType,true),StructField(_c31,StringType,true),StructField(_c32,StringType,true),StructField(_c33,StringType,true),StructField(_c34,StringType,true),StructField(_c35,StringType,true),StructField(_c36,StringType,true),StructField(_c37,StringType,true),StructField(_c38,StringType,true),StructField(_c39,StringType,true)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrêt du context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
